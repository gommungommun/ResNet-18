{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1TgIDiG8IbnKEwBgg1NcTVMmU1TgEpX7Z",
      "authorship_tag": "ABX9TyMlQmCp6ZGg6O0UUVPTQkWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gommungommun/ResNet-18/blob/main/ResNet_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8IN1QPQQI19",
        "outputId": "f71b7ced-748e-411a-aed4-3ee8de92b280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gommungommun/ResNet-18.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sS6FXtPdmcx",
        "outputId": "24216d36-5794-4ef5-83f5-4f375b29fc7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ResNet-18' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from typing import Any, Callable, List, Optional, Type, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor"
      ],
      "metadata": {
        "id": "_FzmZNXYnaCr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Residual Block\n",
        "\n",
        "----\n",
        "\n",
        "* in_planes: 입력 필터 개수\n",
        "\n",
        "* out_planes: 출력 필터 개수\n",
        "\n",
        "* groups: input과 output의 connection을 제어\n",
        "\n",
        "* dilation: 커널 원소간의 거리\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "###1. __init__\n",
        "\n",
        "처음 Normalization Layer가 없는 경우 nn.BatchNorm2d로 지정\n",
        "\n",
        "###2. forward\n",
        "\n",
        "identity 변수에 입력텐서 x를 저장\n",
        "\n",
        "정의해둔 신경망을 거친 뒤 out 과 identity를 더한 후 relu를 거친다.\n",
        "\n",
        "downsampling이 필요한 경우 이를 진행"
      ],
      "metadata": {
        "id": "WJ9n0tf4lWiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 원래 안하지만 편의를 위해 Type Hinting\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ],
      "metadata": {
        "id": "4LGdx8clmsDl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # Normalization Layer\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # downsampling이 필요한 경우 downsample layer를 block에 인자로 넣어주어야함\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity  # residual connection\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "iH7xrhehdupO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Bottleneck class\n",
        "\n",
        "ResNet18 이므로 여기서 쓰이지는 않으나 50 이상 부터는 쓰임\n",
        "\n",
        "---\n",
        "###**[Bottleneck 을 따로 정의하는 이유]**\n",
        "\n",
        "1. 계산 효율성\n",
        "* 1x1컨볼루션을 사용해서 채널 수를 줄였다가 늘림\n",
        "* 이를 통해 3x3컨볼루션의 계산량을 줄일 수 있음\n",
        "\n",
        "2. 모델 크기에 따른 선택\n",
        "\n",
        "3. 메모리 효율성\n",
        "* 중간 레이어의 채널 수를 줄여 메모리 사용을 효율적으로 관리함"
      ],
      "metadata": {
        "id": "XBeTRnVrPQHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    # 확장 계수 (일반적으로 4를 사용)\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        # 1x1 컨볼루션\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "\n",
        "        # 3x3 컨볼루션\n",
        "        self.conv2 = conv3x3(planes, planes, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "\n",
        "        # 1x1 컨볼루션 (채널 확장)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        # 1x1 컨볼루션\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # 3x3 컨볼루션\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # 1x1 컨볼루션 (채널 확장)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        # 다운샘플링이 필요한 경우\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # 스킵 커넥션 더하기\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "emjDxZ7BPP1m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. ResNet class\n",
        "\n",
        "###1. init\n",
        "\n",
        "Normalization Layer가 없는 경우에 생성\n",
        "\n",
        "inplaens, dilation, groups는 각각 64, 1, 1로 고정한다\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "###2. _make_layer\n",
        "\n",
        "residual block을 쌓는다. 필터의 개수는 각 block들을 거치면서 2배씩 늘어나게 된다.\n",
        "\n",
        "모든 block을 거친 후 Adaptive AvgPool2d를 이용해 (n, 512, 1, 1)의 tensor로 만듦\n",
        "\n",
        "그 다음 fc layer를 연결하면 끝임\n",
        "\n",
        "* block: BasicBlock 구조를 사용\n",
        "\n",
        "* plane: input shape\n",
        "\n",
        "* blocks: layer를 반복해서 쌓는 개수\n",
        "\n",
        "* stride, dilate: 고정\n",
        "\n",
        "중간에 downsampling layer를 생성하는 이유는 stride가 1이 아니기 때문에 크기가 줄어들 경우 혹은 plane의 크기가 맞지 않을때 downsampling을 해야하기 때문임\n",
        "\n",
        "----\n",
        "###3. Forward\n",
        "\n",
        "텐서의 사이즈 변화를 나타내기 위해 레이어 별로 사이즈를 출력하도록 함\n"
      ],
      "metadata": {
        "id": "QwwPnQMMrh85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        dropout_rate: float = 0.5\n",
        "    ) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer  # batch norm layer\n",
        "\n",
        "        self.inplanes = 64  # input shape\n",
        "        self.dilation = 1  # dilation fixed\n",
        "        self.groups = 1  # groups fixed\n",
        "\n",
        "        # input block\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # residual blocks\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=False)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=False)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=False)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
        "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "\n",
        "        # downsampling 필요할경우 downsample layer 생성\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes, stride),\n",
        "                norm_layer(planes),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.dilation, norm_layer))\n",
        "        self.inplanes = planes\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                 dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        print('input shape:', x.shape)\n",
        "        x = self.conv1(x)\n",
        "        print('conv1 shape:', x.shape)\n",
        "        x = self.bn1(x)\n",
        "        print('bn1 shape:', x.shape)\n",
        "        x = self.relu(x)\n",
        "        print('relu shape:', x.shape)\n",
        "        x = self.maxpool(x)\n",
        "        print('maxpool shape:', x.shape)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        print('layer1 shape:', x.shape)\n",
        "        x = self.layer2(x)\n",
        "        print('layer2 shape:', x.shape)\n",
        "        x = self.layer3(x)\n",
        "        print('layer3 shape:', x.shape)\n",
        "        x = self.layer4(x)\n",
        "        print('layer4 shape:', x.shape)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        print('avgpool shape:', x.shape)\n",
        "        x = torch.flatten(x, 1)\n",
        "        print('flatten shape:', x.shape)\n",
        "        x = self.dropout(x)\n",
        "        print('Dropout shape: ', x.shape)\n",
        "        x = self.fc(x)\n",
        "        print('fc shape:', x.shape)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "th3oP2KkKOni"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. 결과\n",
        "    Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        3: 입력 채널 수 (RGB 이미지는 3채널)\n",
        "        64: 출력 채널 수\n",
        "        kernel_size=(7, 7): 합성곱 필터의 크기\n",
        "        stride=(2, 2): 필터가 이동하는 간격\n",
        "        padding=(3, 3): 입력 이미지 주변에 추가하는 패딩 크기\n",
        "        bias=False: 편향 사용 여부\n",
        "\n",
        "\n",
        "    BatchNorm2d(64, eps=1e-05, momentum=0.1)\n",
        "        64: 입력 특성 맵의 채널 수\n",
        "        eps: 수치 안정성을 위한 작은 상수\n",
        "        momentum: 이동 평균을 계산할 때 사용되는 모멘텀 값\n",
        "\n",
        "\n",
        "    ReLU(inplace=True)\n",
        "        inplace=True: 입력을 받은 텐서를 직접 수정하여 메모리 효율성 향상\n",
        "\n",
        "\n",
        "    MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        kernel_size=3: 풀링 윈도우의 크기\n",
        "        stride=2: 풀링 윈도우가 이동하는 간격\n",
        "        padding=1: 패딩 크기\n",
        "\n",
        "\n",
        "    BasicBlock 내부 구조:\n",
        "        conv1, conv2: 3x3 컨볼루션 레이어\n",
        "        bn1, bn2: 배치 정규화 레이어\n",
        "        ReLU: 활성화 함수\n",
        "\n",
        "    affine=True\n",
        "        BatchNorm 레이어가 학습 가능한 감마(γ)와 베타(β) 파라미터를 사용할지 여부\n",
        "        True일 경우: y = γ * x_normalized + β 형태로 변환 가능\n",
        "        정규화된 데이터를 다시 스케일링하고 이동시킬 수 있는 유연성 제공\n",
        "        모델의 표현력을 높이는 역할\n",
        "\n",
        "\n",
        "    track_running_stats=True\n",
        "        배치별 평균과 분산의 이동평균(running mean/variance)을 계산하고 저장할지 여부\n",
        "        True일 경우:\n",
        "        학습 중에는 배치별 통계를 사용하면서 이동평균을 업데이트\n",
        "        추론(inference) 시에는 저장된 이동평균을 사용\n",
        "        False일 경우:\n",
        "        항상 현재 배치의 통계만 사용\n",
        "        배치 크기가 1일 때는 문제가 될 수 있음"
      ],
      "metadata": {
        "id": "L7GnCUX_RIlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "print(model)\n",
        "# model_b = ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "# print(model_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAitAVEBJrBN",
        "outputId": "3bf83041-3e53-4b9a-e8c7-9ab28687f77e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(32, 3, 224, 224)\n",
        "print('output shape: ', model(x).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKauXv53L3je",
        "outputId": "543573e5-d343-4e38-d1f9-7a767386764c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([32, 3, 224, 224])\n",
            "conv1 shape: torch.Size([32, 64, 112, 112])\n",
            "bn1 shape: torch.Size([32, 64, 112, 112])\n",
            "relu shape: torch.Size([32, 64, 112, 112])\n",
            "maxpool shape: torch.Size([32, 64, 56, 56])\n",
            "layer1 shape: torch.Size([32, 64, 56, 56])\n",
            "layer2 shape: torch.Size([32, 128, 28, 28])\n",
            "layer3 shape: torch.Size([32, 256, 14, 14])\n",
            "layer4 shape: torch.Size([32, 512, 7, 7])\n",
            "avgpool shape: torch.Size([32, 512, 1, 1])\n",
            "flatten shape: torch.Size([32, 512])\n",
            "Dropout shape:  torch.Size([32, 512])\n",
            "fc shape: torch.Size([32, 1000])\n",
            "output shape:  torch.Size([32, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "suOzpOyIgbtG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}